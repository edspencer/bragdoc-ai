# Assess Corpus Extraction Quality

Evaluate how well achievements were extracted from a corpus snapshot using LLM-as-judge analysis.

## Overview

This skill takes a snapshot file generated by `/run-corpus-extraction` and evaluates the quality of achievement extraction. It identifies specific issues with squash merge handling, large commit handling, branch workflow attribution, achievement completeness, and accuracy.

## Usage

```bash
# Assess a specific snapshot
/assess-corpus-extraction corpus/snapshots/facebook-react/2025-01-20T10-30-00-000Z.json

# Assess with verbose output
/assess-corpus-extraction --verbose corpus/snapshots/facebook-react/2025-01-20T10-30-00-000Z.json
```

## Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `<snapshot-path>` | Path to the snapshot JSON file to assess | Required |
| `--verbose` | Include detailed analysis in output | false |

## Execution Steps

### Step 1: Load and Validate Snapshot

Load the snapshot file and validate its structure:

```typescript
const snapshot = JSON.parse(fs.readFileSync(snapshotPath, 'utf-8'));

// Validate required fields
if (!snapshot.repo || !snapshot.author || !snapshot.extraction) {
  throw new Error('Invalid snapshot format: missing required fields');
}

// Extract key data
const { repo, author, extraction } = snapshot;
const { achievements } = extraction;
```

### Step 2: Load Repository Metadata

Get additional context from the repos manifest:

```typescript
const reposManifest = JSON.parse(fs.readFileSync('corpus/repos-manifest.json', 'utf-8'));

// Find the repo in manifest
let repoMetadata = null;
for (const [category, data] of Object.entries(reposManifest.categories)) {
  const found = data.repos.find(r => r.slug === repo.slug);
  if (found) {
    repoMetadata = { ...found, category };
    break;
  }
}
```

### Step 3: Fetch Commit Data from Cache

Load the raw commit data from the cached repository:

```bash
cd ".corpus-cache/${REPO_SLUG}"

# Get commits by the test author with full details
# Format: hash, author name, author email, timestamp, subject, body (separated by null bytes and record separator)
git log --author="${AUTHOR_EMAIL}" --format="%H%x00%an%x00%ae%x00%at%x00%s%x00%b%x1e" -n 500 > /tmp/commits-raw.txt

# Get file stats for each commit
for hash in $(git log --author="${AUTHOR_EMAIL}" --format="%H" -n 500); do
  echo "=== $hash ==="
  git show --stat --format="" "$hash"
done > /tmp/commits-stats.txt

# Identify squash merge commits (commits with PR references in message)
git log --author="${AUTHOR_EMAIL}" --format="%H %s" -n 500 | grep -E '\(#[0-9]+\)$' > /tmp/squash-commits.txt

# Identify large commits (25+ files changed)
for hash in $(git log --author="${AUTHOR_EMAIL}" --format="%H" -n 500); do
  files_changed=$(git show --stat --format="" "$hash" | grep -c '|')
  if [ "$files_changed" -ge 25 ]; then
    echo "$hash $files_changed"
  fi
done > /tmp/large-commits.txt
```

### Step 4: Analyze Commit Patterns

Parse commit data and identify patterns that affect extraction quality:

```typescript
interface CommitAnalysis {
  totalCommits: number;
  squashMergeCommits: number;
  largeCommits: number;  // 25+ files changed
  multiFeatureCommits: number;  // Commits with multiple bullet points or sections
  poorMessageCommits: number;  // Very short or non-descriptive messages

  // Detailed lists
  squashMergeList: Array<{hash: string; subject: string; prNumber: string}>;
  largeCommitList: Array<{hash: string; subject: string; filesChanged: number}>;
  multiFeatureList: Array<{hash: string; subject: string; features: string[]}>;
  poorMessageList: Array<{hash: string; subject: string; reason: string}>;
}

function analyzeCommits(commits: Commit[]): CommitAnalysis {
  const analysis: CommitAnalysis = {
    totalCommits: commits.length,
    squashMergeCommits: 0,
    largeCommits: 0,
    multiFeatureCommits: 0,
    poorMessageCommits: 0,
    squashMergeList: [],
    largeCommitList: [],
    multiFeatureList: [],
    poorMessageList: [],
  };

  for (const commit of commits) {
    // Check for squash merge (PR reference pattern)
    const prMatch = commit.subject.match(/\(#(\d+)\)$/);
    if (prMatch) {
      analysis.squashMergeCommits++;
      analysis.squashMergeList.push({
        hash: commit.hash,
        subject: commit.subject,
        prNumber: prMatch[1],
      });
    }

    // Check for large commits
    if (commit.filesChanged >= 25) {
      analysis.largeCommits++;
      analysis.largeCommitList.push({
        hash: commit.hash,
        subject: commit.subject,
        filesChanged: commit.filesChanged,
      });
    }

    // Check for multi-feature commits (bullet points, numbered lists, multiple sections)
    const bulletPoints = (commit.body.match(/^[-*â€¢]\s+/gm) || []).length;
    const numberedItems = (commit.body.match(/^\d+\.\s+/gm) || []).length;
    if (bulletPoints >= 3 || numberedItems >= 3) {
      analysis.multiFeatureCommits++;
      const features = extractFeatureList(commit.body);
      analysis.multiFeatureList.push({
        hash: commit.hash,
        subject: commit.subject,
        features,
      });
    }

    // Check for poor commit messages
    if (commit.subject.length < 10 ||
        /^(fix|update|change|wip|temp|test)$/i.test(commit.subject) ||
        commit.subject.toLowerCase().startsWith('wip')) {
      analysis.poorMessageCommits++;
      analysis.poorMessageList.push({
        hash: commit.hash,
        subject: commit.subject,
        reason: commit.subject.length < 10 ? 'Too short' : 'Non-descriptive',
      });
    }
  }

  return analysis;
}
```

### Step 5: Map Achievements to Commits

Create a mapping between extracted achievements and source commits:

```typescript
interface AchievementMapping {
  achievement: Achievement;
  sourceCommits: Commit[];
  confidence: 'high' | 'medium' | 'low';
  issues: string[];
}

function mapAchievementsToCommits(
  achievements: Achievement[],
  commits: Commit[]
): AchievementMapping[] {
  const mappings: AchievementMapping[] = [];

  for (const achievement of achievements) {
    const sourceCommits = commits.filter(c =>
      achievement.sourceIds?.includes(c.hash) ||
      achievement.sourceIds?.includes(c.hash.substring(0, 7))
    );

    const issues: string[] = [];
    let confidence: 'high' | 'medium' | 'low' = 'high';

    // Check if source commits are squash merges
    if (sourceCommits.some(c => /\(#\d+\)$/.test(c.subject))) {
      issues.push('Based on squash merge commit - may compress multiple achievements');
      confidence = 'medium';
    }

    // Check if source commits are large
    if (sourceCommits.some(c => c.filesChanged >= 25)) {
      issues.push('Based on large commit (25+ files) - may contain multiple distinct changes');
      confidence = 'medium';
    }

    // Check if achievement seems too broad
    if (achievement.impact >= 8 && sourceCommits.length === 1) {
      issues.push('High impact claim from single commit - verify scope');
      confidence = 'low';
    }

    mappings.push({
      achievement,
      sourceCommits,
      confidence,
      issues,
    });
  }

  return mappings;
}
```

### Step 6: LLM-as-Judge Assessment

Use the LLM to evaluate extraction quality across multiple dimensions:

```typescript
const assessmentPrompt = `
You are an expert evaluator assessing the quality of achievement extraction from Git commits.

## Repository Context
- **Repository**: ${repo.url} (${repo.slug})
- **Category**: ${repoMetadata?.category || 'unknown'}
- **Merge Strategy**: ${repoMetadata?.mergeStrategy || 'unknown'}
- **Author**: ${author.name} (${author.email})
- **Commits Analyzed**: ${author.commitCount}
- **Achievements Extracted**: ${achievements.length}

## Commit Analysis Summary
- Total Commits: ${commitAnalysis.totalCommits}
- Squash Merge Commits: ${commitAnalysis.squashMergeCommits} (${pct(commitAnalysis.squashMergeCommits, commitAnalysis.totalCommits)}%)
- Large Commits (25+ files): ${commitAnalysis.largeCommits}
- Multi-feature Commits: ${commitAnalysis.multiFeatureCommits}
- Poor Message Commits: ${commitAnalysis.poorMessageCommits}

## Extracted Achievements
${achievements.map((a, i) => `
### Achievement ${i + 1}
- **Title**: ${a.title}
- **Summary**: ${a.summary || 'N/A'}
- **Impact**: ${a.impact}/10
- **Duration**: ${a.eventDuration || 'unspecified'}
- **Source IDs**: ${a.sourceIds?.join(', ') || 'none'}
`).join('\n')}

## Sample Commits (for reference)
${sampleCommits.map(c => `
### Commit ${c.hash.substring(0, 7)}
- **Subject**: ${c.subject}
- **Files Changed**: ${c.filesChanged}
- **Body**: ${c.body?.substring(0, 500) || '(empty)'}
`).join('\n')}

## Squash Merge Commits Detected
${commitAnalysis.squashMergeList.slice(0, 10).map(c =>
  `- ${c.hash.substring(0, 7)}: ${c.subject} (PR #${c.prNumber})`
).join('\n')}

## Large Commits Detected
${commitAnalysis.largeCommitList.slice(0, 10).map(c =>
  `- ${c.hash.substring(0, 7)}: ${c.subject} (${c.filesChanged} files)`
).join('\n')}

## Your Assessment Task

Evaluate the extraction quality on each of the following dimensions. For each dimension:
1. Provide a score from 1-10
2. List specific issues found
3. Provide examples of problematic commits or achievements

### Dimensions to Evaluate

1. **Squash Merge Handling**
   - Did we miss work compressed into single commits?
   - Were multiple achievements hidden inside squash merges?
   - Did we correctly identify when a squash merge represents multiple distinct features?

2. **Large Commit Handling**
   - Did we identify multiple achievements in big commits?
   - Were large refactoring commits properly attributed?
   - Did we over-combine or under-split achievements from large changes?

3. **Branch Workflow Attribution**
   - Was work correctly attributed to the author?
   - Did merge commits cause misattribution?
   - Were feature branches properly accounted for?

4. **Achievement Completeness**
   - Was all major work captured?
   - Are there significant commits not represented in achievements?
   - Did we miss important contributions?

5. **Achievement Accuracy**
   - Are there hallucinated or inflated claims?
   - Do achievement descriptions match the actual commit work?
   - Are impact scores appropriate for the changes made?

6. **Commit Message Quality Impact**
   - Did poor commit messages lead to poor achievements?
   - Were non-descriptive commits handled appropriately?
   - Did we extract meaningful context despite message quality issues?

Respond with a structured JSON assessment:

\`\`\`json
{
  "overallScore": <1-10>,
  "dimensions": {
    "squashMergeHandling": {
      "score": <1-10>,
      "issues": ["issue1", "issue2"],
      "examples": [
        {
          "commitHash": "abc123",
          "problem": "Description of the problem",
          "suggestion": "How to improve"
        }
      ]
    },
    "largeCommitHandling": { ... },
    "branchWorkflowAttribution": { ... },
    "achievementCompleteness": { ... },
    "achievementAccuracy": { ... },
    "commitMessageQualityImpact": { ... }
  },
  "recommendations": [
    "Recommendation 1",
    "Recommendation 2"
  ],
  "notableAchievements": [
    {
      "title": "Achievement title",
      "assessment": "Why this was well/poorly extracted"
    }
  ],
  "missedOpportunities": [
    {
      "commitHash": "abc123",
      "description": "What achievement could have been extracted"
    }
  ]
}
\`\`\`
`;

// Call LLM for assessment
const assessment = await llm.generate({
  prompt: assessmentPrompt,
  maxTokens: 4000,
  temperature: 0.3,  // Lower temperature for more consistent scoring
});
```

### Step 7: Generate Report

Create a structured Markdown report from the assessment:

```typescript
function generateReport(
  snapshot: Snapshot,
  repoMetadata: RepoMetadata,
  commitAnalysis: CommitAnalysis,
  assessment: Assessment
): string {
  const timestamp = new Date().toISOString();

  return `# Extraction Assessment: ${snapshot.repo.slug}

**Date:** ${timestamp.split('T')[0]}
**Author:** ${snapshot.author.name} (${snapshot.author.email})
**Repository:** ${snapshot.repo.url}
**Category:** ${repoMetadata?.category || 'unknown'}
**Merge Strategy:** ${repoMetadata?.mergeStrategy || 'unknown'}
**Commits Analyzed:** ${commitAnalysis.totalCommits}
**Achievements Extracted:** ${snapshot.extraction.achievementCount}

## Overall Quality Score: ${assessment.overallScore}/10

${getScoreEmoji(assessment.overallScore)} ${getScoreDescription(assessment.overallScore)}

## Commit Pattern Analysis

| Pattern | Count | Percentage |
|---------|-------|------------|
| Squash Merge Commits | ${commitAnalysis.squashMergeCommits} | ${pct(commitAnalysis.squashMergeCommits, commitAnalysis.totalCommits)}% |
| Large Commits (25+ files) | ${commitAnalysis.largeCommits} | ${pct(commitAnalysis.largeCommits, commitAnalysis.totalCommits)}% |
| Multi-feature Commits | ${commitAnalysis.multiFeatureCommits} | ${pct(commitAnalysis.multiFeatureCommits, commitAnalysis.totalCommits)}% |
| Poor Message Commits | ${commitAnalysis.poorMessageCommits} | ${pct(commitAnalysis.poorMessageCommits, commitAnalysis.totalCommits)}% |

## Dimension Scores

${Object.entries(assessment.dimensions).map(([dim, data]) => `
### ${formatDimensionName(dim)} (Score: ${data.score}/10)

${data.issues.length > 0 ? `**Issues Identified:**
${data.issues.map(i => `- ${i}`).join('\n')}` : 'No significant issues identified.'}

${data.examples.length > 0 ? `**Examples:**
${data.examples.map(e => `
- **Commit \`${e.commitHash}\`**: ${e.problem}
  - *Suggestion*: ${e.suggestion}
`).join('\n')}` : ''}
`).join('\n')}

## Recommendations

${assessment.recommendations.map((r, i) => `${i + 1}. ${r}`).join('\n')}

## Notable Achievements

${assessment.notableAchievements.length > 0 ?
  assessment.notableAchievements.map(a => `
- **${a.title}**
  - ${a.assessment}
`).join('\n') : 'No notably well or poorly extracted achievements identified.'}

## Missed Opportunities

${assessment.missedOpportunities.length > 0 ?
  assessment.missedOpportunities.map(m => `
- **Commit \`${m.commitHash}\`**: ${m.description}
`).join('\n') : 'No significant missed opportunities identified.'}

## Extracted Achievements Summary

| # | Title | Impact | Duration | Source Commits |
|---|-------|--------|----------|----------------|
${snapshot.extraction.achievements.map((a, i) =>
  `| ${i + 1} | ${a.title.substring(0, 50)}${a.title.length > 50 ? '...' : ''} | ${a.impact}/10 | ${a.eventDuration || '-'} | ${(a.sourceIds || []).length} |`
).join('\n')}

---
*Generated by BragDoc Corpus Assessment Tool*
*Snapshot: ${snapshotPath}*
`;
}

function getScoreEmoji(score: number): string {
  if (score >= 8) return 'ðŸŸ¢';
  if (score >= 6) return 'ðŸŸ¡';
  if (score >= 4) return 'ðŸŸ ';
  return 'ðŸ”´';
}

function getScoreDescription(score: number): string {
  if (score >= 9) return 'Excellent extraction quality';
  if (score >= 8) return 'Very good extraction quality';
  if (score >= 7) return 'Good extraction quality with minor issues';
  if (score >= 6) return 'Acceptable extraction quality';
  if (score >= 5) return 'Below average - notable issues';
  if (score >= 4) return 'Poor extraction quality - significant issues';
  return 'Very poor extraction quality - major problems';
}

function formatDimensionName(dim: string): string {
  return dim
    .replace(/([A-Z])/g, ' $1')
    .replace(/^./, s => s.toUpperCase())
    .trim();
}

function pct(value: number, total: number): string {
  return total > 0 ? ((value / total) * 100).toFixed(1) : '0.0';
}
```

### Step 8: Save Report

Save the report to the reports directory:

```typescript
const repoSlug = snapshot.repo.slug;
const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
const reportDir = `corpus/reports/${repoSlug}`;
const reportPath = `${reportDir}/${timestamp}.md`;

// Create directory if needed
fs.mkdirSync(reportDir, { recursive: true });

// Write report
fs.writeFileSync(reportPath, report);

console.log(`Assessment report saved to: ${reportPath}`);
```

### Step 9: Update Manifest

Add the report reference to the corpus manifest:

```typescript
const manifest = JSON.parse(fs.readFileSync('corpus/manifest.json', 'utf-8'));

manifest.reports.push({
  id: `assessment-${repoSlug}-${timestamp}`,
  type: 'extraction-assessment',
  repo: repoSlug,
  snapshotPath: snapshotPath,
  reportPath: reportPath,
  generatedAt: new Date().toISOString(),
  overallScore: assessment.overallScore,
  dimensionScores: Object.fromEntries(
    Object.entries(assessment.dimensions).map(([k, v]) => [k, v.score])
  ),
});

fs.writeFileSync('corpus/manifest.json', JSON.stringify(manifest, null, 2));
```

## Output Format

The assessment produces a Markdown report with the following structure:

```markdown
# Extraction Assessment: facebook-react

**Date:** 2025-01-20
**Author:** Dan Abramov (gaearon@users.noreply.github.com)
**Repository:** https://github.com/facebook/react
**Category:** monorepos
**Merge Strategy:** squash
**Commits Analyzed:** 500
**Achievements Extracted:** 47

## Overall Quality Score: 7/10

ðŸŸ¡ Good extraction quality with minor issues

## Commit Pattern Analysis

| Pattern | Count | Percentage |
|---------|-------|------------|
| Squash Merge Commits | 450 | 90.0% |
| Large Commits (25+ files) | 35 | 7.0% |
| Multi-feature Commits | 28 | 5.6% |
| Poor Message Commits | 12 | 2.4% |

## Dimension Scores

### Squash Merge Handling (Score: 6/10)

**Issues Identified:**
- 12 squash merge commits appear to compress multiple distinct achievements
- PR descriptions not being used to recover original commit context

**Examples:**
- **Commit `abc123d`**: Contains 3 separate feature implementations in one squash
  - *Suggestion*: Consider splitting when commit message contains multiple bullet points

### Large Commit Handling (Score: 7/10)

**Issues Identified:**
- 5 large commits with 50+ files were merged into single achievements
- 2 refactoring commits incorrectly attributed as features

...

## Recommendations

1. Consider splitting achievements when commit message contains multiple bullet points
2. Look for PR references in squash commits to recover original commit context
3. Apply higher scrutiny to commits with 25+ files changed
4. Consider using GitHub API to fetch original PR descriptions for squash merges

## Notable Achievements

- **Implemented concurrent rendering mode for React fiber reconciler**
  - Well extracted: Correctly identified the scope and impact from commit series

- **Fixed hydration mismatch in SSR**
  - Potentially under-scoped: Multiple related fixes may have been compressed

## Missed Opportunities

- **Commit `def456a`**: Large refactoring commit could have been split into:
  - Code organization improvements
  - Performance optimizations
  - Type safety enhancements

## Extracted Achievements Summary

| # | Title | Impact | Duration | Source Commits |
|---|-------|--------|----------|----------------|
| 1 | Implemented concurrent rendering mode for React... | 9/10 | month | 3 |
| 2 | Fixed hydration mismatch in SSR... | 7/10 | week | 1 |
...

---
*Generated by BragDoc Corpus Assessment Tool*
*Snapshot: corpus/snapshots/facebook-react/2025-01-20T10-30-00-000Z.json*
```

## Error Handling

The skill handles failures gracefully:

1. **Missing snapshot file**: Clear error message with path
2. **Invalid snapshot format**: Validation error with missing fields
3. **Missing cached repo**: Warning and instruction to run extraction first
4. **LLM API errors**: Retry with exponential backoff, fail gracefully with partial report
5. **File system errors**: Log and attempt recovery

```typescript
try {
  // ... assessment logic
} catch (error) {
  if (error.code === 'ENOENT') {
    console.error(`Snapshot file not found: ${snapshotPath}`);
    console.error('Run /run-corpus-extraction first to generate snapshots.');
    process.exit(1);
  }

  if (error.message.includes('rate limit')) {
    console.error('LLM rate limit hit. Waiting and retrying...');
    await sleep(60000);
    // Retry logic
  }

  throw error;
}
```

## Environment Requirements

- **Git**: For accessing cached repository data
- **LLM API Key**: One of OPENAI_API_KEY, ANTHROPIC_API_KEY, or GOOGLE_GENERATIVE_AI_API_KEY
- **Node.js**: For running the assessment
- **Cached Repository**: Must have run `/run-corpus-extraction` first

## Notes

- Assessment uses LLM temperature of 0.3 for more consistent scoring
- Large repositories may take longer due to commit analysis
- Reports are cumulative - running multiple times creates separate reports for tracking over time
- The skill is designed to identify patterns, not prescribe fixes - use recommendations as guidance
